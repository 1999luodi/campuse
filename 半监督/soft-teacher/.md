train_cfg：

type：使用基于迭代次数的训练循环。
max_iters：设置为 80,000 次迭代。
val_interval：每 3,000 次迭代（即每个 Epoch）进行一次验证。
param_scheduler：

LinearLR：在训练初期的 500 次迭代 内，线性增加学习率（预热阶段）。
MultiStepLR：在 48,000 次（60%）和 64,000 次（80%）迭代时，将学习率乘以 0.1，以细化模型参数。
optim_wrapper：

optimizer：使用 SGD 优化器，初始学习率为 0.01，动量为 0.9，权重衰减为 0.0001。
auto_scale_lr：

enable：禁用自动缩放学习率。如果在多 GPU 环境下训练，可以启用并设置合适的 base_batch_size。
# Training schedule for Soft-Teacher with Faster R-CNN on your dataset
train_cfg = dict(
    type='IterBasedTrainLoop',
    max_iters=80000,  # 80,000 次迭代
    val_interval=3000  # 每 3,000 次迭代（即每个 Epoch）验证一次
)
val_cfg = dict(type='TeacherStudentValLoop')
test_cfg = dict(type='TestLoop')

# Learning Rate Scheduler
param_scheduler = [
    dict(
        type='LinearLR',
        start_factor=0.001,
        by_epoch=False,
        begin=0,
        end=500  # 500 次迭代用于学习率预热
    ),
    dict(
        type='MultiStepLR',
        begin=0,
        end=80000,
        by_epoch=False,
        milestones=[48000, 64000],  # 在 60%（48,000）和 80%（64,000）次迭代时降低学习率
        gamma=0.1
    )
]

# Optimizer
optim_wrapper = dict(
    type='OptimWrapper',
    optimizer=dict(type='SGD', lr=0.01, momentum=0.9, weight_decay=0.0001)
)

# Auto Scale LR
auto_scale_lr = dict(enable=False, base_batch_size=2)  # 根据实际 Batch Size 设置
# Training schedule for Faster R-CNN based on Epochs
train_cfg = dict(type='EpochBasedTrainLoop', max_epochs=24, val_interval=1)  # 24 Epochs
val_cfg = dict(type='ValLoop')
test_cfg = dict(type='TestLoop')

# Learning Rate Scheduler
param_scheduler = [
    dict(
        type='LinearLR',
        start_factor=0.001,
        by_epoch=False,
        begin=0,
        end=500  # 500 次迭代用于学习率预热
    ),
    dict(
        type='MultiStepLR',
        begin=0,
        end=24,
        by_epoch=True,
        milestones=[16, 22],  # 在第 16 和 22 个 Epoch 时降低学习率
        gamma=0.1
    )
]

# Optimizer
optim_wrapper = dict(
    type='OptimWrapper',
    optimizer=dict(type='SGD', lr=0.02, momentum=0.9, weight_decay=0.0001)
)

# Auto Scale LR
auto_scale_lr = dict(enable=False, base_batch_size=2)
