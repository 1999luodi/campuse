{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fake_useragent\n",
      "  Downloading fake_useragent-2.0.3-py3-none-any.whl.metadata (17 kB)\n",
      "Requirement already satisfied: importlib-resources>=6.0 in d:\\program\\miniconda\\envs\\d2l\\lib\\site-packages (from fake_useragent) (6.4.0)\n",
      "Requirement already satisfied: zipp>=3.1.0 in d:\\program\\miniconda\\envs\\d2l\\lib\\site-packages (from importlib-resources>=6.0->fake_useragent) (3.18.2)\n",
      "Downloading fake_useragent-2.0.3-py3-none-any.whl (201 kB)\n",
      "   ---------------------------------------- 0.0/201.1 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/201.1 kB ? eta -:--:--\n",
      "   -- ------------------------------------- 10.2/201.1 kB ? eta -:--:--\n",
      "   ----- --------------------------------- 30.7/201.1 kB 435.7 kB/s eta 0:00:01\n",
      "   ----- --------------------------------- 30.7/201.1 kB 435.7 kB/s eta 0:00:01\n",
      "   ----- --------------------------------- 30.7/201.1 kB 435.7 kB/s eta 0:00:01\n",
      "   ----------- --------------------------- 61.4/201.1 kB 326.1 kB/s eta 0:00:01\n",
      "   ----------------- --------------------- 92.2/201.1 kB 374.1 kB/s eta 0:00:01\n",
      "   ----------------------- -------------- 122.9/201.1 kB 423.5 kB/s eta 0:00:01\n",
      "   --------------------------- ---------- 143.4/201.1 kB 425.3 kB/s eta 0:00:01\n",
      "   ----------------------------- -------- 153.6/201.1 kB 416.7 kB/s eta 0:00:01\n",
      "   -------------------------------- ----- 174.1/201.1 kB 388.2 kB/s eta 0:00:01\n",
      "   -------------------------------------- 201.1/201.1 kB 435.8 kB/s eta 0:00:00\n",
      "Installing collected packages: fake_useragent\n",
      "Successfully installed fake_useragent-2.0.3\n"
     ]
    }
   ],
   "source": [
    "# !pip install requests beautifulsoup4 urllib3\n",
    "!pip install fake_useragent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping images for category 'dog'...\n",
      "https://www.google.com/search/index?q=dog\n",
      "<Response [404]>\n",
      "Request failed with status code: 404\n",
      "[]\n",
      "Scraping images for category 'cat'...\n",
      "https://www.google.com/search/index?q=cat\n",
      "<Response [200]>\n",
      "['https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSi3CDdwKIJJ8K9iyUhjsKQJ12cSC6ewaE5thPQNJRzQ3WuxrwmXMWU0qE89iI&s', 'https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcS-2uXwaS15TsFyHVzH2NlkgRZgy7HoshnFflTFOE-b8ie5sSxs5hpOpgnQro0&s', 'https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRlq5ZSaX8mMPdUw0uHnx-5z_fejxWkts0pI2yDjG6LCi2ys9QZ0Q9l1bGbYw&s', 'https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcS-v-d3mL0cJVYf1XaENh-sdSXEuJjdzeKGg2NgEMDv1RNYKpC4W2kgnbM_AJ0&s', 'https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRN6iXUEcg8h8R0tRqZhFOYxXppgmfsA_ep4pO9SjwdKygZ-W2flNvOaCndIw&s', 'https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSHoEbzvcOL3gh-yyZPIeYDDmq0almlNY-DqRo2nnccR7FW0vkUSYPnTMzQfA&s', 'https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQPW6KKCnA1VTUZZzg9KLGsPDSnFkSg25RS8f1ZqpKd9T5TTT8Giy01b_Bv7Q&s', 'https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTJDBqcTIs3uQOnZyvSZb_Keuf2YDrsPnXyp0IJgxmmFUhY5XUcQJfkQ55voWo&s', 'https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSSyHFpRfCn3KnctrxA2AseLxRjU-33UHnsrzK_F8gyeBgnphHrNl6dM0_psAg&s', 'https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSPZO3wz9NL1cRPCfpMNsU-kFT75f2VbB8a3Fq65PdO4wPZAq-adb0l87Beew&s', 'https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRQTFtPsTkPp2xs--jjDs83ok9MFSPT0VBgUaQmBeBppoh-F_pc0NgQs2itsw&s', 'https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRHrQSrZR8c-PzJ6YkSeEuPOHw2sM1ehNdK9nnNs_5wkRkNnRIum2JX5ZOHqA&s', 'https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcT_3FWgxfVLEGc6S9IKmK2mpGeakfjeJUXqBJvWEQqxBS5JlU8t_JVXbvsabRU&s', 'https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQ69nd79h5UXX_pGndqUwNdyNwr2EMVNTH2OGAfo61oO1IdSc-SC-JaZuvuZA&s', 'https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTqRAacyyCUH3be5QnSosbaJx5Mmq2V2AmVX-ygxThqsfhkqc3c9uDuYOTSk68&s', 'https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTRt_Zq0xEyV_1M65wL4Ou6p2rM5j32EsanIzen0V26uDtSIOnzW0KiZCCa8Z4&s', 'https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcR1M6cnFrSeLZ4Mhu6G7M4_1hxuM1Svg-QJ2dnBYv5kM1Kg5GDzx91AZccX-Q&s', 'https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQN2_cAOypMH0H-t7XqaDRBuTYKp-hJ4hiZJsAV2_bSKJeAbcQyalGp2H4OS3w&s', 'https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRT_6mcTIxxFTpkw7xh1Zl9WE6gPfmfh7n7HivmQs_qfcmic-HYRwJQIhbo9G0&s', 'https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcS0DHpXpb_kITI2tLyMZx5-4nqu1jkjsC8ZnA8rU4dhZSnc5inji3tVXYu1KQ&s']\n",
      "Downloaded: cat_train_1.jpg\n",
      "Downloaded: cat_train_2.jpg\n",
      "Downloaded: cat_train_3.jpg\n",
      "Downloaded: cat_train_4.jpg\n",
      "Downloaded: cat_train_5.jpg\n",
      "Downloaded: cat_train_6.jpg\n",
      "Downloaded: cat_train_7.jpg\n",
      "Downloaded: cat_train_8.jpg\n",
      "Downloaded: cat_train_9.jpg\n",
      "Failed to download https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSPZO3wz9NL1cRPCfpMNsU-kFT75f2VbB8a3Fq65PdO4wPZAq-adb0l87Beew&s: <urlopen error [WinError 10054] 远程主机强迫关闭了一个现有的连接。>\n",
      "Downloaded: cat_train_11.jpg\n",
      "Downloaded: cat_train_12.jpg\n",
      "Downloaded: cat_train_13.jpg\n",
      "Downloaded: cat_train_14.jpg\n",
      "Downloaded: cat_val_1.jpg\n",
      "Downloaded: cat_val_2.jpg\n",
      "Downloaded: cat_val_3.jpg\n",
      "Downloaded: cat_test_1.jpg\n",
      "Downloaded: cat_test_2.jpg\n",
      "Downloaded: cat_test_3.jpg\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "from urllib.request import urlretrieve\n",
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urlencode\n",
    "from fake_useragent import UserAgent\n",
    "\n",
    "# 设定爬虫类\n",
    "class ImageScraper:\n",
    "    def __init__(self, base_url, num_images=100, categories=['class_0', 'class_1'], output_dir='dataset'):\n",
    "        \"\"\"\n",
    "        初始化爬虫参数\n",
    "        :param base_url: 爬取的基础URL\n",
    "        :param num_images: 每个类别下载的图片数量\n",
    "        :param categories: 类别名列表\n",
    "        :param output_dir: 数据集保存的根目录\n",
    "        \"\"\"\n",
    "        self.base_url = base_url\n",
    "        self.num_images = num_images\n",
    "        self.categories = categories\n",
    "        self.output_dir = output_dir\n",
    "\n",
    "        # 创建文件夹结构\n",
    "        self.create_folders()\n",
    "\n",
    "    def create_folders(self):\n",
    "        \"\"\"创建存储数据集的文件夹\"\"\"\n",
    "        for category in self.categories:\n",
    "            for split in ['train', 'val', 'test']:\n",
    "                folder_path = os.path.join(self.output_dir, split, category)\n",
    "                if not os.path.exists(folder_path):\n",
    "                    os.makedirs(folder_path)\n",
    "\n",
    "    \n",
    "        \n",
    "    def fetch_image_urls(self, query,idx):\n",
    "        \"\"\"\n",
    "        根据查询获取图片 URL\n",
    "        :param query: 搜索关键字\n",
    "        :return: 图片 URL 列表\n",
    "        \"\"\"\n",
    "        # 构建百度图片搜索 URL\n",
    "        query_params = urlencode({'q': query})\n",
    "        search_url = f'{self.base_url}search/index?{query_params}'\n",
    "        print(search_url)\n",
    "\n",
    "        # 请求头，模拟浏览器访问，防止被封锁\n",
    "        # 请求头，模拟浏览器\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',\n",
    "            'Accept-Encoding': 'gzip, deflate, br',\n",
    "            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8,en-GB;q=0.7,en-US;q=0.6',\n",
    "            'Connection': 'keep-alive',\n",
    "            'Referer': 'https://www.google.com/',\n",
    "            'Upgrade-Insecure-Requests': '1',\n",
    "        }\n",
    "        \n",
    "\n",
    "        # 获取网页内容 \n",
    "        \"\"\"\n",
    "            url的构建是关键，使用浏览器得到相关的url,放入python代码进行下载\n",
    "        \"\"\"\n",
    "        # response = requests.get(search_url, headers=headers)\n",
    "        url=\"\"\n",
    "        if idx==0:#dog\n",
    "            url=search_url\n",
    "            # url=\"https://www.google.com/search?q=dog&sca_esv=d6e9a249458c55ac&biw=1306&bih=776&udm=2&sxsrf=ADLYWIJOjBDfZZjF91i8KnILD6-v9fiRCA%3A1734512853643&ei=1ZBiZ6r6Jr2hnesPgN2fsA8&ved=0ahUKEwjquP_z-7CKAxW9UGcHHYDuB_YQ4dUDCBE&uact=5&oq=dog&gs_lp=EgNpbWciA2RvZzILEAAYgAQYsQMYgwEyCBAAGIAEGLEDMgUQABiABDIFEAAYgAQyBRAAGIAEMgUQABiABDIFEAAYgAQyBRAAGIAEMgUQABiABDIFEAAYgARIsQZQnQRYhwVwAXgAkAEAmAGuAaABwwKqAQMwLjK4AQPIAQD4AQGYAgOgAsgCwgIEECMYJ8ICBhAAGAcYHsICBxAAGIAEGAqYAwCIBgGSBwMxLjKgB9oF&sclient=img\"\n",
    "        elif idx==1:# cat\n",
    "            url=\"https://www.google.com/search?q=cat&sca_esv=d6e9a249458c55ac&biw=1306&bih=776&udm=2&sxsrf=ADLYWILMzMo1FSPdHYLZsnhWM-IaGiLc3A%3A1734512856519&ei=2JBiZ7KtH8CaseMP-uvI0QY&ved=0ahUKEwiy-a71-7CKAxVATWwGHfo1MmoQ4dUDCBE&uact=5&oq=cat&gs_lp=EgNpbWciA2NhdDIEECMYJzILEAAYgAQYsQMYgwEyCxAAGIAEGLEDGIMBMgsQABiABBixAxiDATIIEAAYgAQYsQMyCxAAGIAEGLEDGIMBMgUQABiABDIFEAAYgAQyBRAAGIAEMgsQABiABBixAxiDAUj1BFAAWN0DcAB4AJABAJgB4QOgAc4HqgEJMC4xLjAuMS4xuAEDyAEA-AEBmAIDoALTB8ICDhAAGIAEGLEDGIMBGIoFmAMAkgcJMC4xLjAuMS4xoAepCw&sclient=img\"\n",
    "        response=requests.get(url)\n",
    "        print(response)\n",
    "        if response.status_code == 200:\n",
    "            # 解析网页中的图像标签（如果返回的是 HTML）\n",
    "            img_url_list = img_re.findall(response.text)  # 获取图片链接列表\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            img_tags = soup.find_all('img')\n",
    "            image_urls = []\n",
    "\n",
    "            for img in img_tags:\n",
    "                img_url = img.get('src')\n",
    "                if img_url and img_url.startswith('http'):\n",
    "                    image_urls.append(img_url)\n",
    "                    if len(image_urls) >= self.num_images:\n",
    "                        break\n",
    "\n",
    "            return image_urls\n",
    "        else:\n",
    "            print(f\"Request failed with status code: {response.status_code}\")\n",
    "            return []\n",
    "\n",
    "    def download_images(self, urls, category, split):\n",
    "        \"\"\"根据URL下载图片并保存到对应类别文件夹\"\"\"\n",
    "        save_dir = os.path.join(self.output_dir, split, category)\n",
    "        for idx, url in enumerate(urls):\n",
    "            try:\n",
    "                img_name = f'{category}_{split}_{idx + 1}.jpg'\n",
    "                img_path = os.path.join(save_dir, img_name)\n",
    "                urlretrieve(url, img_path)\n",
    "                print(f\"Downloaded: {img_name}\")\n",
    "                time.sleep(0.5)  # 延时0.5秒，避免被网站封锁\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to download {url}: {e}\")\n",
    "\n",
    "    def scrape_images(self):\n",
    "        \"\"\"执行爬虫任务\"\"\"\n",
    "        for idx, category in enumerate(self.categories):\n",
    "            num_images=self.num_images\n",
    "            print(f\"Scraping images for category '{category}'...\")\n",
    "            image_urls = self.fetch_image_urls(category,idx)\n",
    "            print(image_urls)\n",
    "\n",
    "            # 切分数据为train, val, test\n",
    "            train_urls = image_urls[:int(0.7 * len(image_urls))]\n",
    "            val_urls = image_urls[int(0.7 * len(image_urls)):int(0.85 * len(image_urls))]\n",
    "            test_urls = image_urls[int(0.85 * len(image_urls)):]\n",
    "\n",
    "            # 下载并保存图片\n",
    "            self.download_images(train_urls, category, 'train')\n",
    "            self.download_images(val_urls, category, 'val')\n",
    "            self.download_images(test_urls, category, 'test')\n",
    "\n",
    "# 调用爬虫，设置目标网站URL和类别\n",
    "base_url = 'https://www.google.com/'  # 这里替换成你想抓取的URL\n",
    "categories = ['dog', 'cat']  # 自定义分类名称\n",
    "scraper = ImageScraper(base_url, num_images=50, categories=categories)\n",
    "scraper.scrape_images()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "内容已全部爬取\n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
